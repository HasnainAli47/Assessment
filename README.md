# HEAL-Summ-Lite

A local health-article summarization tool with built-in quality checks, risk scoring, and a human-review escalation rule.

## Setup

```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm


# pull the model (Ollama must be installed)
ollama pull gemma3:4b

# start the server
uvicorn app:app --reload --port 8000
```

Open http://localhost:8000, drop `.txt` articles into `data/raw_articles/`, hit *Process articles*.

## How it works

The pipeline has four stages:

1. **Summarise** — Ollama (`gemma3:4b`) generates a summary whose length scales with the input: the target is 30-50% of the article's word count, clamped to a 25-word floor and 200-word ceiling. A 60-word article gets a ~25-30 word summary; a 500-word article gets ~150-200 words. If the word count is off, the system retries up to twice with a stricter prompt. Non-medical text is blocked by a keyword gate before it ever reaches the model.

2. **Evaluate** — Five independent checks run on every summary:
   - *Readability* (Flesch-Kincaid grade level + reading ease via textstat)
   - *Entity coverage* — what fraction of the original's key entities (people, orgs, dates, numbers) survived into the summary
   - *Hallucination detection* — entities that appear in the summary but not in the source
   - *Numeric consistency* — whether the summary preserved the original statistics
   - *ROUGE scores* — optional, only computed when a hand-written reference summary is available

3. **Score risk** — Six binary flags (missing numbers, low coverage, hard readability, hallucination, toxicity, length violation) are tallied. 0 = Low, 1 = Medium, 2+ = High.

4. **Escalate or pass** — A summary is sent for human review if *any* of these hold: risk is High, entity coverage < 50%, word count is out of range, hallucination detected, or FKGL > 12.

## Why these thresholds

- **FKGL 10 / 12**: The CDC recommends health materials at an 8th-grade level. 10 flags it, 12 forces escalation.
- **Entity coverage 0.6 / 0.5**: Below 60% the summary typically drops something important. Below 50% it's unreliable.
- **Dynamic summary length (30-50% ratio)**: The target word count scales with the input. A fixed 120-word target would force expansion on short articles and under-compress long ones. The 30-50% ratio keeps the summary proportional. Floor of 25 words prevents useless one-liners; ceiling of 200 keeps it readable.

## Known issues

- The model sometimes **drops exact statistics** — e.g. rounding "89.3%" to "approximately 90%". The numeric checker flags this.
- It can **hallucinate entire facts** when input is vague or short. We saw it invent AMI trial data from Lorem Ipsum text. The hallucination flag + keyword gate mitigate this, but the underlying LLM behaviour remains.
- **Entity coverage uses substring matching**, so "North" matches inside "North America". Lemmatisation or fuzzy matching would be better.
- **Numeric regex** misses written-out numbers ("two million"), ranges ("5-10%"), and comma-separated formats ("1,850").
- Entity coverage and hallucination detection are complementary — high coverage does not mean the summary is free of fabricated content.

## Project layout

```
heal_summ_lite/
├── app.py            # FastAPI server + embedded UI
├── summarizer.py     # Ollama calls, retry logic, input validation
├── evaluator.py      # readability, NER, coverage, hallucination, ROUGE
├── risk.py           # flag counting, risk levels, escalation rules
├── utils.py          # shared helpers
├── config.py         # all tuneable constants
├── data/raw_articles/  # input .txt files go here
├── results/            # output JSON + CSV
├── tests/              # pytest suite
├── requirements.txt
└── README.md
```

## Tests

```bash
pytest tests/ -v
```

## Tools used

| Tool | Purpose |
|---|---|
| Ollama (gemma3:4b) | Summary generation (local LLM) |
| spaCy (en_core_web_sm) | Named-entity recognition |
| scispaCy (en_core_sci_sm) | Biomedical NER (optional) |
| textstat | Readability scoring |
| rouge-score | Evaluation metric |
| FastAPI + uvicorn | Web server |

Summaries are generated by an LLM. All validation, scoring, and escalation logic is deterministic code written by the developer.
